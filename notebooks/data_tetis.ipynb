{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constitution des jeux de données pour le Challenge de la Data party**\n",
    "\n",
    "*Les 30 ans de la MTD*\n",
    "\n",
    "# Les jeux de données TETIS publiés\n",
    "\n",
    "Champs d'intérêt:\n",
    "\n",
    "- Localisation de la production : latestVersion.metadataBlocks.geospatial.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DATA_INRAE', 'PREPROD_DATA_INRAE', 'CIRAD']\n"
     ]
    }
   ],
   "source": [
    "credential_file = \"./../credentials.ini\"\n",
    "credential_config = configparser.ConfigParser()\n",
    "credential_config.read(credential_file)\n",
    "print(credential_config.sections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrage API data.inrae.fr\n",
    "API_TOKEN_data_inrae = credential_config['DATA_INRAE']['API_TOKEN']\n",
    "url_rdg = \"https://entrepot.recherche.data.gouv.fr/\"\n",
    "headers_data_inrae = {'Accept': 'application/json',\n",
    "                     'X-Dataverse-key': API_TOKEN_data_inrae\n",
    "}\n",
    "\n",
    "# Paramétrage API https://dataverse.cirad.fr/\n",
    "API_TOKEN_data_cirad = credential_config['CIRAD']['API_TOKEN']\n",
    "url_cirad = \"https://dataverse.cirad.fr/\"\n",
    "headers_data_cirad = {'Accept': 'application/json',\n",
    "                     'X-Dataverse-key': API_TOKEN_data_cirad\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total size of a dataverse\n",
    "def storage_szie_of_dataverse(dataverse, url=url_rdg, header=headers_data_inrae):\n",
    "    url_dataverse = url + \"api/dataverses/\" + dataverse\n",
    "    reponse = requests.get(url_dataverse + \"/storagesize\", headers=header).json()[\"data\"]\n",
    "    return reponse\n",
    "\n",
    "# get dataset ID from a dataverse\n",
    "def datasetID_from_dataverse(dataverse, url=url_rdg, header=headers_data_inrae):\n",
    "    url_dataverse = url + \"/api/dataverses/\" + dataverse\n",
    "    try:\n",
    "        reponse = requests.get(url_dataverse + \"/contents\", headers=header).json()[\"data\"]\n",
    "    except:\n",
    "        print(requests.get(url_dataverse + \"/contents\", headers=header).json())\n",
    "        reponse = {}\n",
    "    return reponse\n",
    "\n",
    "# get all data from a dataverse\n",
    "def datasets_info_from_datasetID(dataverse_content, url=url_rdg, header=headers_data_inrae):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    list_datasets = []\n",
    "    list_dataverses = []\n",
    "    for dataset in dataverse_content:\n",
    "        try: \n",
    "            dataset_info = requests.get(url + \"/api/datasets/\" + str(dataset[\"id\"]), headers=header).json()[\"data\"]\n",
    "            list_datasets.append(dataset_info)\n",
    "        except: # it's not a dataset but a dataverse\n",
    "            list_dataverses.append(str(dataset[\"title\"]).replace(\" \", \"_\"))\n",
    "    return list_datasets, list_dataverses\n",
    "\n",
    "# get view (total and uniques), download (unique & totla) and citation of a dataset\n",
    "def get_metrics_of_datasets(doi):\n",
    "    api_metrics = [\"viewsTotal\", \"viewsUnique\", \"downloadsTotal\", \"downloadsUnique\", \"citations\"]\n",
    "    dic_api_metrics = {}\n",
    "    for metric in api_metrics:\n",
    "        url = url_rdg + \"/api/datasets/:persistentId/makeDataCount/\" + metric + \"?persistentId=\" + doi\n",
    "        try: \n",
    "            reponse = requests.get(url, headers=headers_data_inrae).json()[\"data\"][metric]\n",
    "        except:\n",
    "            reponse = 0\n",
    "        dic_api_metrics[metric] = reponse\n",
    "    print(dic_api_metrics)\n",
    "    return dic_api_metrics\n",
    "\n",
    "def get_metrics_of_datasets_cirad(doi):\n",
    "    api_metrics = [\"viewsTotal\", \"viewsUnique\", \"downloadsTotal\", \"downloadsUnique\", \"citations\"]\n",
    "    dic_api_metrics = {}\n",
    "    for metric in api_metrics:\n",
    "        url = url_cirad + \"/api/datasets/:persistentId/makeDataCount/\" + metric + \"?persistentId=\" + doi\n",
    "        try: \n",
    "            reponse = requests.get(url, headers=headers_data_cirad).json()[\"data\"][metric]\n",
    "        except:\n",
    "            reponse = 0\n",
    "        dic_api_metrics[metric] = reponse\n",
    "    return dic_api_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataverse_to_explore = []\n",
    "list_of_dataverse_to_explore.append(\"tetis\")\n",
    "list_datasets = []\n",
    "# Jump into dataverses inside the dataverse of TETIS!\n",
    "while list_of_dataverse_to_explore:\n",
    "    for dataverse in list_of_dataverse_to_explore:\n",
    "        print(\"RDG: \" + dataverse)\n",
    "        datasetIDs = datasetID_from_dataverse(dataverse)\n",
    "        print(\"\\t Number of objects: \" + str(len(datasetIDs)))\n",
    "        try:\n",
    "            list_datasets_local, list_dataverses_local = datasets_info_from_datasetID(datasetIDs)\n",
    "            list_datasets.extend(list_datasets_local)\n",
    "            list_of_dataverse_to_explore.extend(list_dataverses_local)\n",
    "            print(\"\\t Number of datasets: \" + str(len(list_datasets_local)))\n",
    "            print(\"\\t Number of dataverse: \" + str(len(list_dataverses_local)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(\"Dataverse CIRAD: \" + dataverse)\n",
    "        datasetIDs = datasetID_from_dataverse(dataverse, url=url_cirad, header=headers_data_cirad)\n",
    "        print(\"\\t Number of objects: \" + str(len(datasetIDs)))\n",
    "        try:\n",
    "            list_datasets_local, list_dataverses_local = datasets_info_from_datasetID(datasetIDs, url=url_cirad, header=headers_data_cirad)\n",
    "            list_datasets.extend(list_datasets_local)\n",
    "            list_of_dataverse_to_explore.extend(list_dataverses_local)\n",
    "            print(\"\\t Number of datasets: \" + str(len(list_datasets_local)))\n",
    "            print(\"\\t Number of dataverse: \" + str(len(list_dataverses_local)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        list_of_dataverse_to_explore.remove(dataverse)\n",
    "df_datasets = pd.json_normalize(list_datasets, max_level=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
